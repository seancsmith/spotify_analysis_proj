---
title: "Spotify Analysis"
output: html_notebook
---
(does genre have an impact
run model on before or after adding genre
compare before and after on the same data)
considerations and limitations - geared towards younger people

```{r include=FALSE}
library(spotifyr)
library(tidyverse)
library(modelr)
library(infer)
library(modelr)
library(ggfortify)
library(GGally)
```

```{r include=FALSE}
spotify_data_clean_join <- read_csv("clean_data/spotify_clean_join.csv")
spotify_data <- read_csv("raw_data/spotify_data.csv")
```


### Question

What (audio features) makes a song popular, and has that changed over time?

### Intro

I plan to answer the question, what makes a song on spotify "popular" and how does that change over time. 

For this project I have used a combination of Spotify datasets available on Kaggle, taken from the Spotify Web API. The original dataset contains just under 170,000 rows with 19 column. I have then joined this with another dataset which gave me information on "genres" and "followers". (and time signature) Each row in the dataset represents 1 song, information about that song and many of it's audio features.

I have used RStudio for this project as I found it excellent for model building.

I had no issues surrounding the ethics of using or presenting this data. I had to get authentication to use the Spotify web api but it is readily available for people to view and analyse.

### The Variables

Basic Info
artists <chr> - the artist or artists who appears on the track
track_name <chr> - name of the song
track_id <chr> - a distinct id to represent the song
year <date> - year the track was released
duration_sec <dbl> - duration of the track in seconds
genres <str> - genre of the track, which I have manually aggregated
followers <dbl> - number of followers the artists has
key <fctr> - the key of the song  0-11
mode <fctr> - major or minor key
time_signature <fctr> - time signature of the song
tempo <dbl> - overall tempo of the track in BPM
explicit <fct> - does the track contain explicit content

decade <date> - decade of release
no_of_artists <dbl> - the number of artists appearing on each track
is_popular <lgl> - is the popularity rating 50 or over

All on a scale of 0 - 100
 - acousticness <dbl> - a measure of how acoustic the track is (was it recorded in a live setting or studio)

 - danceability <dbl> - how suitable is the track for dancing, based on rhythm stability, tempo and beat strength.

- energy - measures the intensity and activity level

- instrumentalness - measures how much vocals there are in a track. The higher the number, the less vocals in the track

- liveness - detects the presence of an audience in the recording. The higher the liveness the higher the probability it was recorded live.

- loudness - the overall loudness of a track. Originally in db, scaled to match the other audio features.

- speechiness - measures the presence of spoken word in a track. The closer to 100, the more exclusively speech-like the recording. Rap has higher score than folk.

- valence - the higher the value the more positive a track sounds(e.g. happy, cheerful). The lower the score the more negative it sounds (e.g. sad, angry)

### popularity -  Spotify -  "The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are."


## Hypothesis Tests on Audio Features

Two sample - independent tests

Independent here means that there is no reason to believe that observations in the two samples can be paired in any way. (i.e. there's no reason to believe the danceability in the 60s is the same as the 2010s)

By randomly shuffling (i.e. permuting) the decade labels we lose any relationship that there was between decade and danceability. Think of this shuffling as detaching the labels from rows and then randomly assigning them back to rows. Then we see which of the following occurs:

If there was no relationship in the first place (i.e. they are in fact independent) then randomly shuffling them should have no implication.
If the difference between groups in our sample is much larger than the difference once the labels are shuffled it’s because there is a real difference between the groups, and it’s not just down to sampling variation.

This is an example of the hypothesis tests which were carried out on each audio feature.

H0: The mean danceability in 1960s is the same as the mean danceability in 2010s

Ha: The mean danceability in 1960s is less than the mean danceability in 2010s

H0: The difference in means in 0
Ha: danceability2020 - danceability1960 > 0
```{r include=FALSE}
dance_decade_hyp <- spotify_data_clean_join %>% 
  select(decade, danceability) %>% 
  filter(decade == 1960 | decade == 2010) %>% 
  mutate(decade = as.factor(decade))

null_distribution <- dance_decade_hyp %>% 
  specify(danceability ~ decade) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 5000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("2010", "1960")) 

observed_stat <- dance_decade_hyp %>% 
  specify(danceability ~ decade) %>%
  calculate(stat = "diff in means", order = c("2010", "1960"))

null_distribution %>%
  visualise() +
  shade_p_value(obs_stat = observed_stat, direction = "right")

p_value <- null_distribution %>%
  get_p_value(obs_stat = observed_stat, direction = "right")
```

### How do the audio features change over time

```{r}
spotify_data_clean_pivot <- spotify_data_clean_join %>% 
  pivot_longer(cols = acousticness:valence,
              names_to = "audio_feature",
              values_to = "value") %>% 
  group_by(year, audio_feature) %>% 
  mutate(avg_feature = mean(value))
```


Plot with all Audio Features
```{r}
spotify_data_clean_pivot %>%
  ggplot() +
  aes(x = year, y = avg_feature, group = audio_feature, 
      colour = audio_feature) +
  geom_line(linewidth = 1) +
  ylim(0, 100) +
  labs(x = "Year",
       y = "Value",
       title = "Audio Features Over Time",
       subtitle = "Avg per year",
       colour = "Audio Feature") +
  theme_bw() +
  scale_colour_brewer(palette =  "Dark2") +
  theme(axis.text  = element_text(face = "bold", size = 15, family = "Times"),
        axis.title  = element_text(face = "bold", size = 15, family = "Times"),
        title = element_text(face = "bold", size = 15, family = "Times"),
        legend.text = element_text(face = "bold", size = 10, family = "Times")
  ) 
```

Plot with audio features showing change
```{r}
spotify_data_clean_pivot %>%
    filter(audio_feature == "acousticness" |
           audio_feature == "instrumentalness" |
           audio_feature == "danceability" |
           audio_feature == "energy" |
           audio_feature == "loudness") %>% 
  ggplot() +
  aes(x = year, y = avg_feature, group = audio_feature, 
      colour = audio_feature) +
  geom_line(linewidth = 2) +
  ylim(0, 100) +
  labs(x = "Year",
       y = "Value",
       title = "Audio Features by Release Year",
       subtitle = "Avg per year",
       colour = "Audio Feature") +
  theme_bw() +
  scale_colour_brewer(palette =  "Dark2") +
  theme(axis.text  = element_text(face = "bold", size = 15, family = "Times"),
        axis.title  = element_text(face = "bold", size = 15, family = "Times"),
        title = element_text(face = "bold", size = 15, family = "Times"),
        legend.text = element_text(face = "bold", size = 10, family = "Times")
  ) 

```

```{r}
spotify_data_clean_join %>% 
  #mutate(explicit = as.numeric(explicit)) %>% 
  group_by(year) %>% 
  summarise(total_ex = sum(explicit)) %>% 
  ggplot() +
  aes(x = year, y = total_ex / 20) +
  geom_col(fill = "red4") +
  ylim(0, 50) +
  labs(x = "Year",
       y = "Percentage",
       title = "Percentage of Explicit Songs",
       subtitle = "by year") +
  theme_bw() +
  theme(axis.text  = element_text(face = "bold", size = 15, family = "Times"),
        axis.title  = element_text(face = "bold", size = 15, family = "Times"),
        title = element_text(face = "bold", size = 15, family = "Times"),
        legend.text = element_text(face = "bold", size = 10, family = "Times")
  ) 
```
```{r}
spot_sample %>% 
  ggplot() +
  aes(x = danceability, y = popularity) +
  geom_point() +
  geom_smooth(method = "lm")
```


### Intro To Linear Regression

To help me answer my question of what makes a song "popular" on spotify, I decided to build an explanatory linear regression model. 

This type of analysis is used to determine the strength of the relationship between a response variable y and multiple explanatory variables.

So in this case I will be choosing from all of the variables I have just discussed to explain the popularity of a song on spotify.

y = b0 + b1x1 + b2x2 + b3x3....bnxn

popularity = b0 + b1x1 + b2x2 + b3x3....bnxn

To start this process I plot popularity against each one of my variables or possible explanatory variables and find the strongest correlation.

```{r}
n_data <- nrow(spotify_data_for_modelling)

sample_index <- sample(1:n_data, size = n_data*0.2)

spot_sample <- slice(spotify_data_for_modelling, sample_index)

spot_sample %>% 
  ggplot() +
  aes(x = year, y = popularity) +
  geom_point(alpha = 0.7, colour = "red4") +
  geom_smooth(method = "lm", se = FALSE)
```

My strongest correlation was year (the year the track was released) with a correlation of 0.74. I then add this to my model as my first explanatory variable.

```{r}
model_1a <- lm(popularity ~ year,
               data = train_lm)

summary(model_1a)
```

After running my model I'm looking at 3 factors:
 - The P-value - is this variable making a significant difference. If the P-value is below the significance level of 0.05 then we can reject the null hypothesis and conclude that changes in the 
 
 - The R^2 - is a measure that indicates how much of the variation of popularity is explained by the year
 - The adjusted R^2 - compensates for the addition of variables. So as we're building an explanatory model, we don't want this to drop much lower than the r^2.

```{r}
model_3a <- lm(popularity ~ year + danceability + loudness,
               data = train_lm)

summary(model_3a)
```

```{r}
model_3b <- lm(popularity ~ year + danceability + instrumentalness,
               data = train_lm)

summary(model_3b)
```

```{r}
anova(model_3a, model_2c)
```

Final Model

```{r}
model_6a <- lm(popularity ~ year + danceability + loudness + liveness + explicit,
               data = train_lm)

summary(model_6a)
``` 

So here we have our final model. I stopped adding variables as the Adjusted r^2 started dropping and our multiple r^2 was barely going up.

So we can see that the more recent the release, the more danceability, the louder it is, the less liveness it has. So you don't want a live recording, you want a studio recording with some explicit lyrics chucked in there as well.

All of our P-Values are significant and we have a Multiple r^2 of 0.55, with an adjusted r^2 also of 0.55. This means that 55% of the variance in popularity is explained our other variables. Which  also means that 45% of the variability in the data cannot be explained by this model.

55% isn't a very high proportion but it's not terrible. We are trying to measure the popularity of a song. It's not quite as obvious as if we were trying to explain the value of a house. We might find that the postcode and the number of rooms goes a long way to explaining that value. I think explaining the popularity of a song is bit more complicated than that. If it was easy to know what made a song popular then we'd all be musicians.

Mention human behaviour, mention the basic practice of statistics would call this a moderate effect size. 


```{r}
model_6b <- lm(popularity ~ year + danceability + loudness + liveness + explicit,
               data = test_lm)

summary(model_6b)

``` 

So my model isn't great. What can I do to improve this? Well...

When I was researching how spotify calculate their popularity score I kept coming across this one phrase.

### 50 is the magic number


while the Popularity Index is majorly determined by recent stream count, other factors like save rate, the number of playlists, skip rate, and share rate can indirectly bump up or push down a song’s popularity index. The higher your popularity index, the more likely the algorithm is to recommend you to new listeners, and place you in algorithmic playlists like Release Radar and Discover Weekly. 

Oscar Wilde is famously credited with having written: “Popularity is the one insult I have never suffered.”





